{
  "permissions": {
    "allow": [
      "Bash(python -c \"import streamlit; import pandas; import plotly; import numpy; print(''All dependencies OK'')\")",
      "Bash(pip install -r requirements.txt -q)",
      "Bash(python -m py_compile streamlit_app.py)",
      "Bash(python -c \"\nimport pandas as pd\ndf = pd.read_csv(''data/Speed Dating Data.csv'', encoding=''latin-1'')\n\n# Check dec and dec_o\nprint(''=== dec 和 dec_o 基本统计 ==='')\nprint(f''dec 均值: {df[\"\"dec\"\"].mean():.4f}'')\nprint(f''dec_o 均值: {df[\"\"dec_o\"\"].mean():.4f}'')\nprint()\n\n# 按性别分析\nprint(''=== 按性别分析 ==='')\nmale = df[df[''gender''] == 1]\nfemale = df[df[''gender''] == 0]\nprint(f''男性 dec 均值 (男性给女性的Yes): {male[\"\"dec\"\"].mean():.4f}'')\nprint(f''男性 dec_o 均值 (女性给男性的Yes): {male[\"\"dec_o\"\"].mean():.4f}'')\nprint(f''女性 dec 均值 (女性给男性的Yes): {female[\"\"dec\"\"].mean():.4f}'')\nprint(f''女性 dec_o 均值 (男性给女性的Yes): {female[\"\"dec_o\"\"].mean():.4f}'')\nprint()\n\n# 检查数据结构 - 每条记录代表什么\nprint(''=== 数据结构检查 ==='')\nprint(f''总行数: {len(df)}'')\nprint(f''唯一 iid 数: {df[\"\"iid\"\"].nunique()}'')\nprint(f''唯一 pid 数: {df[\"\"pid\"\"].nunique()}'')\nprint()\n\n# 检查是否每对配对有两条记录（双向）\nsample = df[[''iid'', ''pid'', ''gender'', ''dec'', ''dec_o'', ''match'']].head(20)\nprint(''=== 前20条数据样本 ==='')\nprint(sample.to_string())\n\")",
      "Bash(python -c \"\nimport csv\n\nwith open(''data/Speed Dating Data.csv'', ''r'', encoding=''latin-1'') as f:\n    reader = csv.DictReader(f)\n    rows = list(reader)\n\n# 找到 dec 和 dec_o 的列索引\nprint(''=== 数据结构分析 ==='')\nprint(f''总行数: {len(rows)}'')\n\n# 统计 dec 和 dec_o\ndec_sum = 0\ndec_o_sum = 0\ndec_count = 0\ndec_o_count = 0\n\nmale_dec_sum = 0\nmale_dec_count = 0\nfemale_dec_sum = 0\nfemale_dec_count = 0\n\nfor row in rows:\n    dec = row.get(''dec'', '''')\n    dec_o = row.get(''dec_o'', '''')\n    gender = row.get(''gender'', '''')\n    \n    if dec and dec != '''':\n        try:\n            dec_val = float(dec)\n            dec_sum += dec_val\n            dec_count += 1\n            if gender == ''1'':\n                male_dec_sum += dec_val\n                male_dec_count += 1\n            elif gender == ''0'':\n                female_dec_sum += dec_val\n                female_dec_count += 1\n        except: pass\n    \n    if dec_o and dec_o != '''':\n        try:\n            dec_o_sum += float(dec_o)\n            dec_o_count += 1\n        except: pass\n\nprint(f''dec 均值: {dec_sum/dec_count:.4f} (n={dec_count})'')\nprint(f''dec_o 均值: {dec_o_sum/dec_o_count:.4f} (n={dec_o_count})'')\nprint()\nprint(f''男性(gender=1) dec 均值: {male_dec_sum/male_dec_count:.4f}'')\nprint(f''女性(gender=0) dec 均值: {female_dec_sum/female_dec_count:.4f}'')\nprint()\n\n# 检查前10条数据\nprint(''=== 前10条数据 ==='')\nprint(''iid, pid, gender, dec, dec_o, match'')\nfor row in rows[:10]:\n    print(f\"\"{row[''iid'']}, {row[''pid'']}, {row[''gender'']}, {row[''dec'']}, {row[''dec_o'']}, {row[''match'']}\"\")\n\")",
      "Bash(python -c \"\nimport csv\nwith open(''data/Speed Dating Data.csv'', ''r'', encoding=''latin-1'') as f:\n    reader = csv.DictReader(f)\n    headers = reader.fieldnames\n    print(''Total columns:'', len(headers))\n    print()\n    # 打印我们需要的字段\n    needed = [''age'', ''gender'', ''race'', ''mn_sat'', ''tuition'', ''field_cd'', ''career_c'', ''income'', ''zipcode'', ''date'', ''go_out'', ''goal'',\n              ''attr_o'', ''sinc_o'', ''intel_o'', ''fun_o'', ''amb_o'', ''shar_o'',\n              ''attr1_1'', ''sinc1_1'', ''intel1_1'', ''fun1_1'', ''amb1_1'', ''shar1_1'',\n              ''imprace'', ''imprelig'', ''expnum'', ''match_es'', ''wave'', ''iid'']\n    print(''Checking needed fields:'')\n    for f in needed:\n        if f in headers:\n            print(f''  {f}: Found'')\n        else:\n            print(f''  {f}: NOT FOUND'')\n\")",
      "Bash(python -m py_compile pages/1_Match_Landscape.py)",
      "Bash(python -m py_compile pages/2_Participants_Portrait.py)",
      "Bash(pip install scikit-learn umap-learn -q)",
      "Bash(python -c \"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n\n# 加载数据\ndf = pd.read_csv(''data/Speed Dating Data.csv'', encoding=''latin-1'')\n\n# 选择字段\ndemo_cols = [''iid'', ''gender'', ''age'', ''race'', ''field_cd'', ''career_c'', ''date'', ''go_out'', ''goal'', ''imprace'', ''imprelig'', ''wave'']\npref_cols = [''attr1_1'', ''sinc1_1'', ''intel1_1'', ''fun1_1'', ''amb1_1'', ''shar1_1'']\nself_cols = [''attr3_1'', ''sinc3_1'', ''intel3_1'', ''fun3_1'', ''amb3_1'']\n\nall_cols = demo_cols + pref_cols + self_cols\navailable_cols = [c for c in all_cols if c in df.columns]\n\n# 聚合到参与者级别\nparticipant_df = df.groupby(''iid'').first().reset_index()[available_cols]\n\n# 数据清洗\nfor col in [''age'', ''date'', ''go_out'', ''imprace'', ''imprelig''] + pref_cols + self_cols:\n    if col in participant_df.columns:\n        participant_df[col] = pd.to_numeric(participant_df[col], errors=''coerce'')\n        participant_df[col] = participant_df[col].fillna(participant_df[col].median())\n\n# One-hot编码\nfor col in [''race'', ''field_cd'', ''career_c'', ''goal'']:\n    if col in participant_df.columns:\n        participant_df[col] = pd.to_numeric(participant_df[col], errors=''coerce'')\n        dummies = pd.get_dummies(participant_df[col], prefix=col, dummy_na=False)\n        participant_df = pd.concat([participant_df, dummies], axis=1)\n\n# 构建特征矩阵\nfeature_cols = []\nfor col in [''age'', ''gender'', ''date'', ''go_out'', ''imprace'', ''imprelig'']:\n    if col in participant_df.columns:\n        feature_cols.append(col)\nfor col in pref_cols + self_cols:\n    if col in participant_df.columns:\n        feature_cols.append(col)\nfor col in participant_df.columns:\n    if any(col.startswith(f''{cat}_'') for cat in [''race'', ''field_cd'', ''career_c'', ''goal'']):\n        feature_cols.append(col)\n\nX = participant_df[feature_cols].copy()\nvalid_mask = ~X.isna().any(axis=1)\nX = X[valid_mask]\n\n# 标准化\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(f''样本数: {len(X_scaled)}, 特征数: {X_scaled.shape[1]}'')\nprint()\n\n# 评估不同K值\nprint(''='' * 60)\nprint(f''{''K'':>3} | {''Inertia'':>12} | {''Silhouette'':>10} | {''CH Index'':>12} | {''DB Index'':>10}'')\nprint(''='' * 60)\n\nresults = []\nfor k in range(2, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(X_scaled)\n    \n    inertia = kmeans.inertia_\n    silhouette = silhouette_score(X_scaled, labels)\n    ch_score = calinski_harabasz_score(X_scaled, labels)\n    db_score = davies_bouldin_score(X_scaled, labels)\n    \n    results.append({\n        ''k'': k,\n        ''inertia'': inertia,\n        ''silhouette'': silhouette,\n        ''ch_score'': ch_score,\n        ''db_score'': db_score\n    })\n    \n    print(f''{k:>3} | {inertia:>12.1f} | {silhouette:>10.4f} | {ch_score:>12.1f} | {db_score:>10.4f}'')\n\nprint(''='' * 60)\nprint()\n\n# 找最佳K\nbest_silhouette_k = max(results, key=lambda x: x[''silhouette''])[''k'']\nbest_ch_k = max(results, key=lambda x: x[''ch_score''])[''k'']\nbest_db_k = min(results, key=lambda x: x[''db_score''])[''k'']\n\nprint(''指标说明:'')\nprint(''  - Inertia: 越小越好 (肘部法则找拐点)'')\nprint(''  - Silhouette: 越大越好 (-1到1，越接近1越好)'')\nprint(''  - CH Index (Calinski-Harabasz): 越大越好'')\nprint(''  - DB Index (Davies-Bouldin): 越小越好'')\nprint()\nprint(f''最佳K推荐:'')\nprint(f''  - 基于 Silhouette Score: K = {best_silhouette_k}'')\nprint(f''  - 基于 CH Index: K = {best_ch_k}'')\nprint(f''  - 基于 DB Index: K = {best_db_k}'')\n\")",
      "Bash(python -c \"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n\n# 加载数据\ndf = pd.read_csv(''data/Speed Dating Data.csv'', encoding=''latin-1'')\n\n# 选择字段\ndemo_cols = [''iid'', ''gender'', ''age'', ''race'', ''field_cd'', ''career_c'', ''date'', ''go_out'', ''goal'', ''imprace'', ''imprelig'', ''wave'']\npref_cols = [''attr1_1'', ''sinc1_1'', ''intel1_1'', ''fun1_1'', ''amb1_1'', ''shar1_1'']\nself_cols = [''attr3_1'', ''sinc3_1'', ''intel3_1'', ''fun3_1'', ''amb3_1'']\n\nall_cols = demo_cols + pref_cols + self_cols\navailable_cols = [c for c in all_cols if c in df.columns]\n\n# 聚合到参与者级别\nparticipant_df = df.groupby(''iid'').first().reset_index()[available_cols]\n\n# 数据清洗\nfor col in [''age'', ''date'', ''go_out'', ''imprace'', ''imprelig''] + pref_cols + self_cols:\n    if col in participant_df.columns:\n        participant_df[col] = pd.to_numeric(participant_df[col], errors=''coerce'')\n        participant_df[col] = participant_df[col].fillna(participant_df[col].median())\n\n# One-hot编码\nfor col in [''race'', ''field_cd'', ''career_c'', ''goal'']:\n    if col in participant_df.columns:\n        participant_df[col] = pd.to_numeric(participant_df[col], errors=''coerce'')\n        dummies = pd.get_dummies(participant_df[col], prefix=col, dummy_na=False)\n        participant_df = pd.concat([participant_df, dummies], axis=1)\n\n# 构建特征矩阵\nfeature_cols = []\nfor col in [''age'', ''gender'', ''date'', ''go_out'', ''imprace'', ''imprelig'']:\n    if col in participant_df.columns:\n        feature_cols.append(col)\nfor col in pref_cols + self_cols:\n    if col in participant_df.columns:\n        feature_cols.append(col)\nfor col in participant_df.columns:\n    if any(col.startswith(cat + ''_'') for cat in [''race'', ''field_cd'', ''career_c'', ''goal'']):\n        feature_cols.append(col)\n\nX = participant_df[feature_cols].copy()\nvalid_mask = ~X.isna().any(axis=1)\nX = X[valid_mask]\n\n# 标准化\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(''样本数: %d, 特征数: %d'' % (len(X_scaled), X_scaled.shape[1]))\nprint()\n\n# 评估不同K值\nprint(''='' * 65)\nprint(''%3s | %12s | %10s | %12s | %10s'' % (''K'', ''Inertia'', ''Silhouette'', ''CH Index'', ''DB Index''))\nprint(''='' * 65)\n\nresults = []\nfor k in range(2, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(X_scaled)\n    \n    inertia = kmeans.inertia_\n    silhouette = silhouette_score(X_scaled, labels)\n    ch_score = calinski_harabasz_score(X_scaled, labels)\n    db_score = davies_bouldin_score(X_scaled, labels)\n    \n    results.append({\n        ''k'': k,\n        ''inertia'': inertia,\n        ''silhouette'': silhouette,\n        ''ch_score'': ch_score,\n        ''db_score'': db_score\n    })\n    \n    print(''%3d | %12.1f | %10.4f | %12.1f | %10.4f'' % (k, inertia, silhouette, ch_score, db_score))\n\nprint(''='' * 65)\nprint()\n\n# 找最佳K\nbest_silhouette_k = max(results, key=lambda x: x[''silhouette''])[''k'']\nbest_ch_k = max(results, key=lambda x: x[''ch_score''])[''k'']\nbest_db_k = min(results, key=lambda x: x[''db_score''])[''k'']\n\nprint(''指标说明:'')\nprint(''  - Inertia: 越小越好 (肘部法则找拐点)'')\nprint(''  - Silhouette: 越大越好 (-1到1，越接近1越好)'')\nprint(''  - CH Index (Calinski-Harabasz): 越大越好'')\nprint(''  - DB Index (Davies-Bouldin): 越小越好'')\nprint()\nprint(''最佳K推荐:'')\nprint(''  - 基于 Silhouette Score: K = %d'' % best_silhouette_k)\nprint(''  - 基于 CH Index: K = %d'' % best_ch_k)\nprint(''  - 基于 DB Index: K = %d'' % best_db_k)\n\")",
      "Bash(python -c \"\nimport pandas as pd\ndf = pd.read_csv(''data/Speed Dating Data.csv'', encoding=''latin-1'')\nprint(''Columns:'', df.columns[:15].tolist())\nprint(''\\nFirst 10 rows of key columns:'')\nprint(df[[''iid'', ''pid'', ''gender'', ''wave'', ''match'', ''dec'', ''dec_o'']].head(10))\nprint(''\\n--- Check for duplicates ---'')\n# Check if a meeting has two records (A-B and B-A)\nsample = df[df[''wave'']==1][[''iid'', ''pid'', ''match'', ''dec'', ''dec_o'']].head(20)\nprint(sample)\nprint(''\\nTotal rows:'', len(df))\nprint(''Unique (iid, pid) pairs:'', len(df.groupby([''iid'', ''pid''])))\nprint(''Unique unordered pairs:'', len(df.groupby(df.apply(lambda r: tuple(sorted([r[''iid''], r[''pid'']])), axis=1)))\n\")",
      "Bash(python -c \"\nimport pandas as pd\ndf = pd.read_csv(''data/Speed Dating Data.csv'', encoding=''latin-1'')\nprint(''Columns:'', df.columns[:15].tolist())\nprint(''\\nFirst 10 rows of key columns:'')\nprint(df[[''iid'', ''pid'', ''gender'', ''wave'', ''match'', ''dec'', ''dec_o'']].head(10))\nprint(''\\n--- Check for duplicates ---'')\n# Check if a meeting has two records (A-B and B-A)\nsample = df[df[''wave'']==1][[''iid'', ''pid'', ''match'', ''dec'', ''dec_o'']].head(20)\nprint(sample)\nprint(''\\nTotal rows:'', len(df))\nprint(''Unique (iid, pid) pairs:'', len(df.groupby([''iid'', ''pid''])))\n# Unique unordered pairs\ndf[''pair_key''] = df.apply(lambda r: tuple(sorted([int(r[''iid'']), int(r[''pid''])])) if pd.notna(r[''iid'']) and pd.notna(r[''pid'']) else None, axis=1)\nprint(''Unique unordered pairs:'', df[''pair_key''].nunique())\n\")",
      "Bash(pip install pandas --quiet)",
      "Bash(python -c \"\nimport pandas as pd\ndf = pd.read_csv(''data/Speed Dating Data.csv'', encoding=''latin-1'')\nprint(''First 15 rows of key columns:'')\nprint(df[[''iid'', ''pid'', ''gender'', ''wave'', ''match'', ''dec'', ''dec_o'']].head(15).to_string())\nprint(''\\nTotal rows:'', len(df))\n# Check unique pairs\ndf_clean = df.dropna(subset=[''iid'', ''pid''])\ndf_clean[''pair_key''] = df_clean.apply(lambda r: tuple(sorted([int(r[''iid'']), int(r[''pid''])])), axis=1)\nprint(''Unique unordered pairs:'', df_clean[''pair_key''].nunique())\nprint(''Unique ordered pairs:'', len(df_clean.groupby([''iid'', ''pid''])))\n# Check if each pair has 2 records\npair_counts = df_clean.groupby(''pair_key'').size()\nprint(''Pair record distribution:'')\nprint(pair_counts.value_counts())\n\")",
      "Bash(where python)",
      "Bash(C:UsersHXR15547301099Miniconda3python.exe -c \"\nimport pandas as pd\ndf = pd.read_csv(''data/Speed Dating Data.csv'', encoding=''latin-1'')\nprint(''First 15 rows of key columns:'')\nprint(df[[''iid'', ''pid'', ''gender'', ''wave'', ''match'', ''dec'', ''dec_o'']].head(15).to_string())\nprint(''\\nTotal rows:'', len(df))\n# Check unique pairs\ndf_clean = df.dropna(subset=[''iid'', ''pid''])\ndf_clean[''pair_key''] = df_clean.apply(lambda r: tuple(sorted([int(r[''iid'']), int(r[''pid''])])), axis=1)\nprint(''Unique unordered pairs:'', df_clean[''pair_key''].nunique())\nprint(''Unique ordered pairs:'', len(df_clean.groupby([''iid'', ''pid''])))\n# Check if each pair has 2 records\npair_counts = df_clean.groupby(''pair_key'').size()\nprint(''\\nPair record distribution:'')\nprint(pair_counts.value_counts().to_string())\n# Check matches\nprint(''\\nMatch analysis:'')\ndf_clean_match = df_clean[df_clean[''match'']==1]\nprint(''Total match=1 records:'', len(df_clean_match))\nprint(''Unique match pairs:'', df_clean_match[''pair_key''].nunique())\n\")",
      "Bash(D:D1Python3912python.exe -c \"import pandas; print(''pandas ok'')\")",
      "Bash(python -c \"import pandas; print(''pandas ok'')\")",
      "Bash(pip3 list)",
      "Bash(python -c \"\nimport ast\ntry:\n    with open(''pages/1_Match_Landscape.py'', ''r'', encoding=''utf-8'') as f:\n        content = f.read()\n    ast.parse(content)\n    print(''Syntax OK'')\nexcept SyntaxError as e:\n    print(f''Syntax Error: {e}'')\n\")",
      "Bash(python -c \"\nimport ast\ntry:\n    with open(''pages/1_Match_Landscape.py'', ''r'', encoding=''utf-8'') as f:\n        content = f.read()\n    ast.parse(content)\n    print(''Syntax OK'')\nexcept SyntaxError as e:\n    print(f''Syntax Error at line {e.lineno}: {e.msg}'')\n    print(f''Text: {e.text}'')\n\")",
      "Bash(python -c \"\nimport ast\ntry:\n    with open(''pages/1_Match_Landscape.py'', ''r'', encoding=''utf-8'') as f:\n        content = f.read()\n    ast.parse(content)\n    print(''Syntax OK'')\nexcept SyntaxError as e:\n    print(f''Syntax Error at line {e.lineno}: {e.msg}'')\n\")",
      "Bash(python -c \"import pandas as pd; df = pd.read_csv(''data/Speed Dating Data.csv'', encoding=''latin-1''); print(''Columns:'', list(df.columns)); print(''Shape:'', df.shape)\")",
      "Bash(python -m py_compile pages/0_Overview_Dashboard.py)",
      "Bash(python -m py_compile:*)"
    ]
  }
}
